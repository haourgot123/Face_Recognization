# I. Giá»›i thiá»‡u vá» ArcFace

NhÆ° chÃºng ta Ä‘Ã£ tÃ¬m hiá»ƒu vá» mÃ´ hÃ¬nh FaceNet kÃ¨m vá»›i `Triplet Loss Function` (hÃ m loss bá»™ ba) vá»›i 3 bá»©c áº£nh: anchor, positive, negative. Má»¥c tiÃªu cá»§a hÃ m `triplet loss` lÃ  giáº£m khoáº£ng cÃ¡ch giá»¯a 2 vector embedding cá»§a nhá»¯ng ngÆ°á»i trong cÃ¹ng 1 class vÃ  tÄƒng khoáº£ng cÃ¡ch giá»¯a 2 vector embedding cá»§a 2 ngÆ°á»i á»Ÿ 2 class khÃ¡c nhau.
Vá» cÆ¡ báº£n thÃ¬ triplet-loss based method gáº·p pháº£i 2 váº¥n Ä‘á» cÆ¡ báº£n nhÆ° sau:

- Má»™t lÃ  náº¿u dataset cÃ ng lá»›n, sáº½ cÃ³ sá»± bÃ¹ng ná»• vá» sá»‘ bá»™ ba triplet samples, dáº«n Ä‘áº¿n sá»± láº·p láº¡i Ä‘Ã¡ng ká»ƒ cÃ¡c bÆ°á»›c.
- Hai lÃ m viá»‡c tÃ¬m ra cÃ¡c bá»™ semi-hard samples khÃ´ng pháº£i lÃ  chuyá»‡n Ä‘Æ¡n giáº£n.

**Sau Ä‘Ã¢y chÃºng ta sáº½ tÃ¬m hiá»ƒu vá» hÃ m máº¥t mÃ¡t cá»§a ArcFace Ä‘Æ°á»£c láº¥y Ã½ tÆ°á»Ÿng tá»« hÃ m máº¥t mÃ¡t cá»§a `softmax`** 

# II. Softmax Loss Function
## 1. HÃ m Softmax
Trong bÃ i toÃ¡n phÃ¢n loáº¡i nhá»‹ phÃ¢n chÃºng ta thÆ°á»ng sá»­ dá»¥ng cÃ¡c thuáº­t toÃ¡n nhÆ° `Logistic Regression` Ä‘á»ƒ phÃ¢n loáº¡i 2 nhÃ£n 1 vÃ  0. Váº­y vá»›i bÃ i toÃ¡n phÃ¢n loáº¡i nhiá»u hÆ¡n 2 lá»›p chÃºng ta cÃ³ thá»ƒ Ã¡p dá»¥ng Ä‘Æ°á»£c `Logistic Regression` hay khÃ´ng?
![alt text](image/image1.png)
HÃ£y nhÃ¬n vÃ o bá»©c áº£nh dÆ°á»›i Ä‘Ã¢y, Ä‘áº§u vÃ o `x` Ä‘Æ°á»£c káº¿t ná»‘i vá»›i 1 lá»›p fully connected Ä‘á»ƒ táº¡o ra `z`. CÃ¡c giÃ¡ trá»‹ z nÃ y Ä‘Æ°á»£c Ä‘Æ°a qua hÃ m `sigmoid` má»™t cÃ¡ch Ä‘á»™c láº­p vÃ  khÃ´ng há» cÃ³ má»™t má»‘i liÃªn káº¿t nÃ o vá»›i nhau, ta cÅ©ng khÃ´ng Ä‘áº£m báº£o Ä‘Æ°á»£c ráº±ng tá»•ng xÃ¡c suáº¥t á»Ÿ cÃ¡c Ä‘áº§u ra báº±ng 1. NhÆ° váº­y Náº¿u ta khai thÃ¡c Ä‘Æ°á»£c má»‘i quan há»‡ giá»¯a cÃ¡c $z_i$ thÃ¬ káº¿t quáº£ cá»§a bÃ i toÃ¡n classification sáº½ tá»‘t hÆ¡n.
Váº­y chÃºng ta cáº§n 1 mÃ´ hÃ¬nh xÃ¡c suáº¥t sao cho vá»›i má»—i input $x, a_i$ thá»ƒ hiá»‡n xÃ¡c suáº¥t Ä‘á»ƒ input Ä‘Ã³ rÆ¡i vÃ o class thá»© i. Váº­y Ä‘iá»u kiá»‡n lÃ  $a_i$ pháº£i dÆ°Æ¡ng vÃ  tá»•ng cÃ¡c $a_i$ báº±ng 1. VÃ  thÃªm 1 Ä‘iá»u kiá»‡n tá»± nhiÃªn ná»¯a lÃ  : $z_i = w_i^T.x$ cÃ ng lá»›n thÃ¬ xÃ¡c suáº¥t input `x` thuá»™c vÃ o lá»›p Ä‘Ã³ cÃ ng cao. NhÆ° váº­y chÃºng ta cáº§n sá»­ dá»¥ng 1 hÃ m Ä‘á»“ng biáº¿n á»Ÿ Ä‘Ã¢y. 
HÃ m Softmax thoáº£ mÃ£n Ä‘Æ°á»£c cÃ¡c yÃªu cáº§u trÃªn Ä‘Ã¢y:

$a_i = \frac{e^{z_i}}{\sum_{j = 1}^{C} e^{z_j}}\ \ \ \forall i = 1, 2, 3, ..., C$

## 2. HÃ m máº¥t máº¥t (Loss Fucntion)

HÃ m máº¥t mÃ¡t Ä‘Æ°á»£c xÃ¢y dá»±ng Ä‘á»ƒ tá»‘i thiá»ƒu hoÃ¡ sá»± khÃ¡c biá»‡t giá»¯a Ä‘áº§u vÃ o vÃ  Ä‘áº§u ra. Má»™t lá»±a chá»n Ä‘áº§u tiÃªn cÃ³ thá»ƒ nghÄ© tá»›i lÃ :
$J(W) = \sum_{i = 1}^{N} ||a_i - y||^2_2$
Tuy nhiÃªn Ä‘Ã¢y chÆ°a pháº£i lÃ  má»™t lá»±a chá»n tá»‘t. Khi Ä‘Ã¡nh giÃ¡ sá»± khÃ¡c nhau (hay khoáº£ng cÃ¡ch) giá»¯a hai phÃ¢n bá»‘ xÃ¡c suáº¥t (probability distributions), chÃºng ta cÃ³ má»™t Ä‘áº¡i lÆ°á»£ng Ä‘o Ä‘áº¿m khÃ¡c hiá»‡u quáº£ hÆ¡n. Äáº¡i lÆ°á»£ng Ä‘Ã³ cÃ³ tÃªn lÃ  cross entropy.

Cross Entropy giá»¯a 2 Ä‘áº¡i lÆ°á»£ng `p`, `q` Ä‘Æ°á»£c Ä‘inh nghÄ©a nhÆ° sau:

$H(p, q) = -\sum_{i = 1}^{C} p_ilogq_i$

Vá»›i Softmax Regression, trong trÆ°á»ng há»£p cÃ³ C class, loss giá»¯a Ä‘áº§u ra dá»± Ä‘oÃ¡n vÃ  Ä‘áº§u ra thá»±c sá»± Ä‘Æ°á»£c tÃ­nh nhÆ° sau:
$J(W) = -\sum_{j = 1}^{C}y_{ij}log(a_{ij})$
Ta sáº½ káº¿t há»£p táº¥t cáº£ cÃ¡c cáº·p dá»± liá»‡u $x_i, y_i$ Ä‘á»ƒ táº¡o ra hÃ m máº¥t mÃ¡t cho Softmax Regression:

$J(W) = -\sum_{i = 1}^{N}\sum_{j = 1}^{C} y_{ij}log(a_{ij})$

$=  -\sum_{i = 1}^{N}\sum_{j = 1}^{C} y_{ij}log(\frac{exp(w_j^Tx_i)}{\sum_{k = 1}^{C}exp(w_k^Tx_i)})$ 

Äá»ƒ Ä‘Æ¡n giáº£n hoÃ¡ vá»›i $y_{ij}$ lÃ  vector onehot. Tá»©c lÃ  náº¿u nhÃ£n Ä‘Ãºng sáº½ lÃ  1, ngÆ°á»£c láº¡i lÃ  0. Ta sáº½ tÃ³m gÃ³n cÃ´ng thá»©c trÃªn láº¡i nhÆ° sau:
$J(W) = - \sum_{i = 1}^{N}log(\frac{exp(w_{y_i}^Tx_i)}{\sum_{j = 1}^{C}exp(w_j^Tx_i)})$

PhÃ¢n tÃ­ch 1 chÃºt: $W^Tx = ||W||.||x||.cos(\theta_j)$. ÄÃ¢y lÃ  cÃ´ng thá»©c tÃ­ch vÃ´ hÆ°á»›ng nhÃ© !
CÃ¡c báº¡n cÃ³ thá»ƒ tham kháº£o link paper táº¡i [Ä‘Ã¢y nhÃ©](https://arxiv.org/pdf/1801.07698.pdf).
Äá»ƒ Ä‘Æ¡n giáº£n, weight Ä‘Æ°á»£c normalized = 1 báº±ng cÃ¡ch sá»­ dá»¥ng L2 Normalization. CÃ¡c vector Ä‘áº·c trÆ°ng cÅ©ng Ä‘Æ°á»£c chuáº©n hoÃ¡ L2 Normalization vÃ  re-scaled vá» báº±ng s. Sau bÆ°á»›c nÃ y quÃ¡ trÃ¬nh predictions chá»‰ phá»¥ thuá»™c vÃ o gÃ³c giá»¯a features vÃ  weight.

$L2 = -\frac{1}{N}\sum_{i=1}^{N}log(\frac{e^{scos\theta_{y_i}}}{e^{scos\theta_{y_i}} + \sum_{j = 1, j\neq y_i}^{N}e^{scos\theta_j}})$

Theo nhÆ° paper thÃ¬ cÃ¡c embedding feature Ä‘Æ°á»£c phÃ¢n phá»‘i xung quanh má»—i feature center trÃªn 1 hypershpere (hÃ¬nh cáº§u trong khÃ´ng gian n chiá»u), trong paper Ä‘á» xuáº¥t thÃªm 1 additive angular margin penalty m giá»¯a $x_i$ vÃ  $W_{y_i}$
BiÃªn GÃ³c TÄƒng CÆ°á»ng: Há»‡ sá»‘ ğ‘š lÃ m tÄƒng gÃ³c $\theta$ giá»¯a vector embedding vÃ  vector trá»ng sá»‘ cá»§a lá»›p Ä‘Ãºng. Äiá»u nÃ y tÆ°Æ¡ng Ä‘Æ°Æ¡ng vá»›i viá»‡c yÃªu cáº§u mÃ´ hÃ¬nh há»c pháº£i táº¡o ra cÃ¡c vector embedding vá»›i cosine similarity nhá» hÆ¡n cho cÃ¡c máº«u thuá»™c cÃ¹ng má»™t lá»›p.

Giáº£m Cosine Similarity: Khi gÃ³c Î¸ tÄƒng (do thÃªm ğ‘š), giÃ¡ trá»‹ cosine similarity sáº½ giáº£m (vÃ¬ $cos(\theta + m) < cos(\theta)$)

Äiá»u nÃ y cÃ³ nghÄ©a lÃ  Ä‘á»ƒ Ä‘áº¡t Ä‘Æ°á»£c cÃ¹ng má»™t xÃ¡c suáº¥t dá»± Ä‘oÃ¡n cho lá»›p Ä‘Ãºng (trong hÃ m softmax), mÃ´ hÃ¬nh pháº£i há»c cÃ¡c vector embedding sao cho chÃºng hÆ°á»›ng cÃ ng gáº§n vector trá»ng sá»‘ cá»§a lá»›p Ä‘Ãºng cÃ ng tá»‘t.

TÄƒng Kháº£ NÄƒng PhÃ¢n TÃ¡ch: Viá»‡c giáº£m cosine similarity yÃªu cáº§u mÃ´ hÃ¬nh pháº£i táº¡o ra cÃ¡c vector embedding cÃ³ sá»± khÃ¡c biá»‡t rÃµ rÃ ng giá»¯a cÃ¡c lá»›p. Khi cÃ¡c lá»›p cÃ³ cosine similarity tháº¥p hÆ¡n, cÃ¡c vector embedding cá»§a cÃ¡c máº«u thuá»™c cÃ¡c lá»›p khÃ¡c nhau sáº½ trá»Ÿ nÃªn xa nhau hÆ¡n trong khÃ´ng gian vector. Äiá»u nÃ y dáº«n Ä‘áº¿n viá»‡c cÃ¡c lá»›p Ä‘Æ°á»£c phÃ¢n tÃ¡ch rÃµ rÃ ng hÆ¡n.

![alt text](image/image2.png)
NhÃ¬n vÃ o hÃ¬nh cÃ³ thá»ƒ tháº¥y sá»± khÃ¡c biá»‡t giá»¯a softmax vÃ  arcface. CÃ¡c class trong arcface Ä‘Æ°á»£c phÃ¢n tÃ¡ch má»™t cÃ¡ch rÃµ rÃ ng hÆ¡n.
